filebeat.inputs:
- type: docker
  combine_partial: true
  containers:
    paths:
    - '/usr/share/dockerlogs/data/*/*.log'
    stream: "all"
    ids:
      - "*"
  exclude_files: ['\.gz$']
  ignore_older: 10m

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

processors:
  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
  - decode_json_fields:
    fields: ["log", "message"]
    target: ""
    # overwrite existing target elasticsearch fields while decoding json fields    
    overwrite_keys: true
  - add_docker_metadata:
    host: "unix:///var/run/docker.sock"

# setup filebeat to send output to elasticsearch
output.elasticsearch:
  hosts: ["172.20.20.207:9200"]
  index: "app1nahda"

setup.template.name: "app1nahda"
setup.template.pattern: "app1nahda-*"

# setup.kibana:
#   host: "localhost:5601"

# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.level: warning
logging.to_files: false
logging.to_syslog: true
loggins.metrice.enabled: false
logging.files:
  path: /var/log/filebeat
  # Name of files where logs will write
  name: filebeat.log
  # Log File will rotate if reach max size and will create new file. Default value is 10MB
  rotateeverybytes: 10485760 # = 10MB
  keepfiles: 7
  permissions: 0644
ssl.verification_mode: none